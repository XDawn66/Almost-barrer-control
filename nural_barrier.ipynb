{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 397.6137,  852.4254, -122.5904,   10.0000])\n",
      "tensor([ 393.7723,  861.6581, -112.5905,   10.0000])\n",
      "19999\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/safe_data_set.pkl\", \"rb\") as f:\n",
    "    safe_data = pickle.load(f)  # Load the Pickle file\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "safe_data = torch.tensor(safe_data, dtype=torch.float32)\n",
    "# Use safe data in training\n",
    "states_safe = safe_data  # Use this in your training loop\n",
    "\n",
    "with open(\"data/unsafe_data_set.pkl\", \"rb\") as f2:\n",
    "    unsafe_data = pickle.load(f2)  # Load the Pickle file\n",
    "\n",
    "unsafe_data = torch.tensor(unsafe_data, dtype=torch.float32)\n",
    "states_unsafe = unsafe_data\n",
    "next_states_safe = states_safe[1:]  # All but the first state\n",
    "states_safe = states_safe[:-1]  # All but the last state\n",
    "print(states_safe[0])\n",
    "print(next_states_safe[0])\n",
    "print(len(states_safe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarrierFunctionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BarrierFunctionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.tanh(self.fc1(x))\n",
    "        return self.fc2(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    \"\"\"ReLU function to enforce barrier constraints.\"\"\"\n",
    "    return torch.maximum(x, torch.tensor(0.0))\n",
    "\n",
    "def barrier_loss(B_s, B_u, Lf_B_s, gamma, ws=1.0, wu=1.0, wl=1.0):\n",
    "    \"\"\"\n",
    "    Computes the barrier function loss L(θ).\n",
    "\n",
    "    Args:\n",
    "        B_s: Barrier function output for safe states (Tensor of shape [Ns]).\n",
    "        B_u: Barrier function output for unsafe states (Tensor of shape [Nu]).\n",
    "        Lf_B_s: Lie derivative of B(x) for safe states (Tensor of shape [Ns]).\n",
    "        gamma: Weight for the Lie derivative constraint.\n",
    "        ws, wu, wl: Weights for different loss terms.\n",
    "\n",
    "    Returns:\n",
    "        Total loss value.\n",
    "    \"\"\"\n",
    "    Ns = B_s.shape[0]  # Number of safe samples\n",
    "    Nu = B_u.shape[0]  # Number of unsafe samples\n",
    "\n",
    "    # Compute each term of the loss\n",
    "    loss_safety = ws * torch.mean(phi(-B_s))   # First term\n",
    "    loss_usability = wu * torch.mean(phi(B_u)) # Second term\n",
    "    loss_invariance = wl * torch.mean(phi(-Lf_B_s - gamma * B_s))  # Third term\n",
    "\n",
    "    # Combine the terms\n",
    "    total_loss = loss_safety + loss_usability + loss_invariance\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Lf_B_approx(barrier_net, states, next_states, delta_t):\n",
    "    \"\"\"\n",
    "    Approximates the Lie derivative Lf B(x) using sampled trajectories.\n",
    "\n",
    "    Args:\n",
    "        barrier_net: Neural network for the barrier function.\n",
    "        states: Current states (Tensor of shape [Ns, state_dim]).\n",
    "        next_states: Next states (Tensor of shape [Ns, state_dim]).\n",
    "        delta_t: Time difference between consecutive states.\n",
    "\n",
    "    Returns:\n",
    "        Approximation of Lf B(x).\n",
    "    \"\"\"\n",
    "    # Compute B(s) and B(s0)\n",
    "    B_s = barrier_net(states)\n",
    "    B_next = barrier_net(next_states)\n",
    "\n",
    "    # Approximation: (B(s') - B(s)) / delta_t\n",
    "    Lf_B = (B_next - B_s) / delta_t\n",
    "    Lf_B_scalar = Lf_B.mean()\n",
    "    return Lf_B_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\495214539.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint6.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BarrierFunctionNet(\n",
       "  (fc1): Linear(in_features=4, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load('checkpoint9.pth')\n",
    "\n",
    "# Reinitialize the model and optimizer\n",
    "barrier_net = BarrierFunctionNet(4)\n",
    "optimizer = Adam(barrier_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Load the state_dict for the model and optimizer\n",
    "barrier_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Get the last epoch and loss if needed\n",
    "start_epoch = checkpoint['epoch']\n",
    "last_loss = checkpoint['loss']\n",
    "\n",
    "# Set the model to evaluation mode or training mode\n",
    "barrier_net.train()  # or barrier_net.eval() if you want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 139350 with loss: 0.0014598967973142862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\324380667.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4  # Example input dimension for state\n",
    "gamma = 1    # Weight for invariance term\n",
    "ws, wu, wl = 1, 1, 1  # Loss weights\n",
    "delta_t = 0.1  # Time step\n",
    "\n",
    "# Initialize the barrier function network and optimizer\n",
    "barrier_net = BarrierFunctionNet(input_dim)\n",
    "optimizer = Adam(barrier_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Training data (replace with actual safe/unsafe state data)\n",
    "Ns, Nu = 20000, 20000  # Number of safe and unsafe samples\n",
    "\n",
    "\n",
    "# Initialize the barrier function network and optimizer\n",
    "barrier_net = BarrierFunctionNet(input_dim)\n",
    "optimizer = Adam(barrier_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Check if a checkpoint exists and load it\n",
    "checkpoint_path = 'checkpoint9.pth'\n",
    "start_epoch = 1  # default start epoch if no checkpoint is found\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    barrier_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Continue from the next epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch} with loss: {checkpoint['loss']}\")\n",
    "\n",
    "# Training loop (starting from the right epoch)\n",
    "for epoch in range(start_epoch, 2000):\n",
    "    # Forward pass: Compute B(x) for safe and unsafe states\n",
    "    B_s = barrier_net(states_safe)\n",
    "    B_u = barrier_net(states_unsafe)\n",
    "\n",
    "    # Compute Lf B(x) approximation for safe states\n",
    "    Lf_B_s = compute_Lf_B_approx(barrier_net, states_safe, next_states_safe, delta_t)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = barrier_loss(B_s, B_u, Lf_B_s, gamma, ws, wu, wl)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save model and optimizer state every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,  # current epoch\n",
    "            'model_state_dict': barrier_net.state_dict(),  # model parameters\n",
    "            'optimizer_state_dict': optimizer.state_dict(),  # optimizer parameters\n",
    "            'loss': loss.item(),  # last loss value\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_safe_states(barrier_net, states_safe):\n",
    "    margin = 300  # Allowable violations\n",
    "    barrier_values = barrier_net(states_safe)  # Compute B(x) for all safe states\n",
    "    num_violations = torch.sum(barrier_values < 0).item()  # Count violations\n",
    "    \n",
    "    if num_violations > margin:\n",
    "        print(f\"Condition 1 violated: {num_violations} states have B(x) < 0, exceeding the margin of {margin}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Condition 1 satisfied: {num_violations} violations within the margin of {margin}\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_unsafe_states(barrier_net, states_unsafe):\n",
    "    margin = 300  # Allowable violations\n",
    "    barrier_values = barrier_net(states_unsafe)  # Compute B(x) for all unsafe states\n",
    "    num_violations = torch.sum(barrier_values > 0).item()  # Count violations\n",
    "    if num_violations > margin:\n",
    "        print(f\"Condition 1 violated: {num_violations} states have B(x) > 0, exceeding the margin of {margin}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Condition 1 satisfied: {num_violations} violations within the margin of {margin}\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_around_boundary(states, threshold=0.1, delta=0.05):\n",
    "    # Create a grid around the boundary where B(x) = 0, checking zero crossing\n",
    "    grid_states = []\n",
    "    \n",
    "    # Loop over each state in the list of states\n",
    "    for state in states:\n",
    "        # Convert state to a tensor if it's not already\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        # Compute the barrier function for this state\n",
    "        barrier_value = barrier_net(state_tensor).mean().item()  # Take the mean of the output vector\n",
    "        \n",
    "        # If B(x) is near 0 (within threshold), create a grid around this state\n",
    "        if np.abs(barrier_value) < threshold:  \n",
    "            # Create a grid around this state within a range of delta\n",
    "            for dx in np.linspace(-delta, delta, num=5):\n",
    "                # Apply dx element-wise to each dimension of the state\n",
    "                grid_state = state + np.array([dx] * len(state))  # Add dx to each dimension of state\n",
    "                grid_states.append(grid_state.tolist())  # Ensure grid_state is a list\n",
    "                \n",
    "    # Convert grid states to a tensor and return\n",
    "    return torch.tensor(grid_states, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lie_derivative(barrier_net, state, dynamics_model, control_input=None):\n",
    "    if control_input is None:\n",
    "        control_input = np.array([0.0, 0.0])  # Default control input: [acceleration, steering] = [1, 0]\n",
    "    \n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Compute barrier function value B(x)\n",
    "    B_x = barrier_net(state_tensor)\n",
    "    \n",
    "    # Ensure B_x is scalar\n",
    "    B_x = B_x.mean() \n",
    "    \n",
    "    # Compute gradient of B(x) w.r.t. state\n",
    "    B_x.backward()\n",
    "    grad_B_x = state_tensor.grad  # ∇B(x)\n",
    "\n",
    "    f_x = torch.tensor(dynamics_model(state_tensor, control_input), dtype=torch.float32)\n",
    "\n",
    "    # Compute Lie derivative Lf B(x) = ∇B(x) ⋅ f(x)\n",
    "    Lf_B = torch.dot(grad_B_x, f_x).item()  # Use .item() to extract the scalar value\n",
    "\n",
    "    return Lf_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tensor_rows(tensor):\n",
    "    \"\"\"Remove duplicate rows in a PyTorch tensor\"\"\"\n",
    "    return torch.unique(tensor, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lie_derivative(barrier_net, grid_states, dynamics_model, delta_t=0.1, B_U_threshold=0.01, B_L_threshold=-0.01):\n",
    "    default_control_input = np.array([0.0, 0.0])  # Example: [acceleration, steering] = [0, 0]\n",
    "    violating_states = []\n",
    "    \n",
    "    for state in grid_states:\n",
    "        # Ensure state is a numpy array or a list\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        # Compute the barrier function B(x) and its Lie derivative Lf B(x)\n",
    "        B_x = barrier_net(state_tensor).mean().item()  # Barrier function value\n",
    "        Lf_B = compute_lie_derivative(barrier_net, state_tensor, dynamics_model, delta_t, control_input=default_control_input)\n",
    "        \n",
    "        # Check if the conditions for violation are met\n",
    "        B_U = max(B_x, 0)  # Assuming B_U(x) corresponds to max(B(x), 0)\n",
    "        B_L = min(B_x, 0)  # Assuming B_L(x) corresponds to min(B(x), 0)\n",
    "        \n",
    "        # All three conditions must be satisfied for a state to be a violation\n",
    "        if B_U > B_U_threshold and B_L < B_L_threshold and Lf_B <= 0:\n",
    "            violating_states.append(state)\n",
    "\n",
    "    print(f\"Condition 3 violated: {len(violating_states)} states found.\")\n",
    "    \n",
    "    return violating_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamics_model(state, control_input):\n",
    "    if isinstance(state, torch.Tensor):\n",
    "        state_values = state.detach().numpy()  # Convert the tensor to a numpy array\n",
    "    else:\n",
    "        state_values = state  # If it's already a numpy array, use it directly\n",
    "    \n",
    "    # Unpack the state values\n",
    "    x, y, theta, v = state_values[0], state_values[1], state_values[2], state_values[3]\n",
    "    a, delta = control_input  # Extract control inputs (acceleration, steering angle)\n",
    "    #print(x,y)\n",
    "    L = 2.5  # Wheelbase length (for example)\n",
    "    \n",
    "    # Compute the next state using the dynamics equations\n",
    "    dx = v * np.cos(theta) # Change in x position\n",
    "    dy = v * np.sin(theta) # Change in y position\n",
    "    delta = (v / L) * np.tan(delta) # Change in heading (yaw rate)\n",
    "    dv = a  # Change in velocity (acceleration)\n",
    "    #print(f\"dx: {dx}, dy: {dy}, dtheta: {dtheta}, dv: {dv}\")\n",
    "    return np.array([dx, dy, delta, dv], dtype=float)  # Explicitly set the dtype to ensure consistent types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 1 satisfied: B(x) >= 0 for all x in X_s\n",
      "Condition 2 violated: B(x) is non-negative for some x in X_u\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\3441715354.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\3441715354.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  grid_state = state + np.array([dx] * len(state))  # Add dx to each dimension of state\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\406496864.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\1941408205.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 3 violated: 0 states found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Condition 1: Verify safe states\n",
    "verify_safe_states(barrier_net, states_safe)\n",
    "\n",
    "# Condition 2: Verify unsafe states\n",
    "verify_unsafe_states(barrier_net, states_unsafe)\n",
    "\n",
    "# Condition 3: Verify Lie derivative for boundary states\n",
    "#does not work when using the create_grid_around_boundary2 function\n",
    "grid_states = create_grid_around_boundary(states_safe, threshold=0.5, delta=0.1)\n",
    "#grid_states = create_grid_around_boundary(states_safe, threshold=0.1, delta=0.05)\n",
    "violating_states = verify_lie_derivative(barrier_net, grid_states, dynamics_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_labeling(states, labels, test_states, k=15):\n",
    "    # Fit kNN classifier on the states with known labels\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(states, labels)\n",
    "    \n",
    "    # Predict labels for the test states\n",
    "    predicted_labels = knn.predict(test_states)\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m violating_states_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([state\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m violating_states])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use kNN to predict the labels for the violating states\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m violating_labels \u001b[38;5;241m=\u001b[39m \u001b[43mknn_labeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviolating_states_np\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mknn_labeling\u001b[1;34m(states, labels, test_states, k)\u001b[0m\n\u001b[0;32m      5\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(states, labels)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict labels for the test states\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_labels\n",
      "File \u001b[1;32mf:\\Conda2\\envs\\NCBF_IHI\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:274\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(probabilities, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Conda2\\envs\\NCBF_IHI\\lib\\site-packages\\sklearn\\neighbors\\_base.py:838\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    836\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 838\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mf:\\Conda2\\envs\\NCBF_IHI\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mf:\\Conda2\\envs\\NCBF_IHI\\lib\\site-packages\\sklearn\\utils\\validation.py:1093\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1088\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1092\u001b[0m             )\n\u001b[1;32m-> 1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare training data (safe and unsafe states)\n",
    "training_states = np.vstack([states_safe, states_unsafe])  # Stack them together\n",
    "training_labels = np.array([1] * len(states_safe) + [0] * len(states_unsafe))  # Safe = 1, Unsafe = 0\n",
    "\n",
    "# Convert violating states into a NumPy array\n",
    "violating_states_np = np.array([state.detach().numpy() for state in violating_states])\n",
    "\n",
    "# Use kNN to predict the labels for the violating states\n",
    "violating_labels = knn_labeling(training_states, training_labels, violating_states_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'violating_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num_safe_violations \u001b[38;5;241m=\u001b[39m (\u001b[43mviolating_labels\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Count where label is 1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_unsafe_violations \u001b[38;5;241m=\u001b[39m (violating_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Count where label is 0\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafe violating states: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_safe_violations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'violating_labels' is not defined"
     ]
    }
   ],
   "source": [
    "num_safe_violations = (violating_labels == 1).sum().item()  # Count where label is 1\n",
    "num_unsafe_violations = (violating_labels == 0).sum().item()  # Count where label is 0\n",
    "\n",
    "print(f\"Safe violating states: {num_safe_violations}\")\n",
    "print(f\"Unsafe violating states: {num_unsafe_violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 1 satisfied: 146 violations within the margin of 300\n",
      "Condition 1 violated: 967 states have B(x) > 0, exceeding the margin of 300\n",
      "Resuming training from epoch 164550 with loss: 0.00127696234267205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\1366368615.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164560, Loss: 0.0008\n",
      "Epoch 164570, Loss: 0.0008\n",
      "Epoch 164580, Loss: 0.0006\n",
      "Epoch 164590, Loss: 0.0006\n",
      "Epoch 164600, Loss: 0.0006\n",
      "Epoch 164610, Loss: 0.0005\n",
      "Epoch 164620, Loss: 0.0007\n",
      "Epoch 164630, Loss: 0.0005\n",
      "Epoch 164640, Loss: 0.0006\n",
      "Epoch 164650, Loss: 0.0008\n",
      "Condition 1 violated: 8165 states have B(x) < 0, exceeding the margin of 300\n",
      "Condition 1 satisfied: 8 violations within the margin of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\3441715354.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\3441715354.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  grid_state = state + np.array([dx] * len(state))  # Add dx to each dimension of state\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\406496864.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "C:\\Users\\Zhenyu\\AppData\\Local\\Temp\\ipykernel_3540\\1941408205.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 164650 with loss: 0.0008123196312226355\n",
      "Epoch 164660, Loss: 0.0076\n",
      "Epoch 164670, Loss: 0.0063\n",
      "Epoch 164680, Loss: 0.0022\n",
      "Epoch 164690, Loss: 0.0013\n",
      "Epoch 164700, Loss: 0.0009\n",
      "Epoch 164710, Loss: 0.0008\n",
      "Epoch 164720, Loss: 0.0007\n",
      "Epoch 164730, Loss: 0.0006\n",
      "Epoch 164740, Loss: 0.0005\n",
      "Epoch 164750, Loss: 0.0006\n",
      "Condition 1 satisfied: 85 violations within the margin of 300\n",
      "Condition 1 violated: 473 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 164750 with loss: 0.0005775589379481971\n",
      "Epoch 164760, Loss: 0.0005\n",
      "Epoch 164770, Loss: 0.0006\n",
      "Epoch 164780, Loss: 0.0004\n",
      "Epoch 164790, Loss: 0.0005\n",
      "Epoch 164800, Loss: 0.0006\n",
      "Epoch 164810, Loss: 0.0004\n",
      "Epoch 164820, Loss: 0.0006\n",
      "Epoch 164830, Loss: 0.0004\n",
      "Epoch 164840, Loss: 0.0006\n",
      "Epoch 164850, Loss: 0.0011\n",
      "Condition 1 satisfied: 9 violations within the margin of 300\n",
      "Condition 1 violated: 889 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 164850 with loss: 0.0010950859868898988\n",
      "Epoch 164860, Loss: 0.0005\n",
      "Epoch 164870, Loss: 0.0004\n",
      "Epoch 164880, Loss: 0.0004\n",
      "Epoch 164890, Loss: 0.0004\n",
      "Epoch 164900, Loss: 0.0004\n",
      "Epoch 164910, Loss: 0.0006\n",
      "Epoch 164920, Loss: 0.0004\n",
      "Epoch 164930, Loss: 0.0004\n",
      "Epoch 164940, Loss: 0.0004\n",
      "Epoch 164950, Loss: 0.0003\n",
      "Condition 1 violated: 553 states have B(x) < 0, exceeding the margin of 300\n",
      "Condition 1 satisfied: 273 violations within the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 164950 with loss: 0.0003153459110762924\n",
      "Epoch 164960, Loss: 0.0004\n",
      "Epoch 164970, Loss: 0.0004\n",
      "Epoch 164980, Loss: 0.0005\n",
      "Epoch 164990, Loss: 0.0004\n",
      "Epoch 165000, Loss: 0.0005\n",
      "Epoch 165010, Loss: 0.0004\n",
      "Epoch 165020, Loss: 0.0004\n",
      "Epoch 165030, Loss: 0.0004\n",
      "Epoch 165040, Loss: 0.0003\n",
      "Epoch 165050, Loss: 0.0003\n",
      "Condition 1 violated: 557 states have B(x) < 0, exceeding the margin of 300\n",
      "Condition 1 satisfied: 238 violations within the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165050 with loss: 0.00027518911520019174\n",
      "Epoch 165060, Loss: 0.0004\n",
      "Epoch 165070, Loss: 0.0003\n",
      "Epoch 165080, Loss: 0.0005\n",
      "Epoch 165090, Loss: 0.0005\n",
      "Epoch 165100, Loss: 0.0003\n",
      "Epoch 165110, Loss: 0.0004\n",
      "Epoch 165120, Loss: 0.0005\n",
      "Epoch 165130, Loss: 0.0004\n",
      "Epoch 165140, Loss: 0.0003\n",
      "Epoch 165150, Loss: 0.0003\n",
      "Condition 1 satisfied: 86 violations within the margin of 300\n",
      "Condition 1 violated: 338 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165150 with loss: 0.0003181520733051002\n",
      "Epoch 165160, Loss: 0.0006\n",
      "Epoch 165170, Loss: 0.0003\n",
      "Epoch 165180, Loss: 0.0003\n",
      "Epoch 165190, Loss: 0.0006\n",
      "Epoch 165200, Loss: 0.0004\n",
      "Epoch 165210, Loss: 0.0003\n",
      "Epoch 165220, Loss: 0.0005\n",
      "Epoch 165230, Loss: 0.0004\n",
      "Epoch 165240, Loss: 0.0003\n",
      "Epoch 165250, Loss: 0.0005\n",
      "Condition 1 satisfied: 89 violations within the margin of 300\n",
      "Condition 1 violated: 371 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165250 with loss: 0.00047653348883613944\n",
      "Epoch 165260, Loss: 0.0004\n",
      "Epoch 165270, Loss: 0.0003\n",
      "Epoch 165280, Loss: 0.0008\n",
      "Epoch 165290, Loss: 0.0007\n",
      "Epoch 165300, Loss: 0.0007\n",
      "Epoch 165310, Loss: 0.0004\n",
      "Epoch 165320, Loss: 0.0004\n",
      "Epoch 165330, Loss: 0.0006\n",
      "Epoch 165340, Loss: 0.0006\n",
      "Epoch 165350, Loss: 0.0014\n",
      "Condition 1 satisfied: 152 violations within the margin of 300\n",
      "Condition 1 violated: 653 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165350 with loss: 0.0014408929273486137\n",
      "Epoch 165360, Loss: 0.0006\n",
      "Epoch 165370, Loss: 0.0007\n",
      "Epoch 165380, Loss: 0.0200\n",
      "Epoch 165390, Loss: 0.0150\n",
      "Epoch 165400, Loss: 0.0059\n",
      "Epoch 165410, Loss: 0.0018\n",
      "Epoch 165420, Loss: 0.0012\n",
      "Epoch 165430, Loss: 0.0007\n",
      "Epoch 165440, Loss: 0.0006\n",
      "Epoch 165450, Loss: 0.0007\n",
      "Condition 1 satisfied: 81 violations within the margin of 300\n",
      "Condition 1 violated: 503 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165450 with loss: 0.0006788127357140183\n",
      "Epoch 165460, Loss: 0.0004\n",
      "Epoch 165470, Loss: 0.0004\n",
      "Epoch 165480, Loss: 0.0004\n",
      "Epoch 165490, Loss: 0.0004\n",
      "Epoch 165500, Loss: 0.0003\n",
      "Epoch 165510, Loss: 0.0003\n",
      "Epoch 165520, Loss: 0.0003\n",
      "Epoch 165530, Loss: 0.0003\n",
      "Epoch 165540, Loss: 0.0006\n",
      "Epoch 165550, Loss: 0.0004\n",
      "Condition 1 satisfied: 175 violations within the margin of 300\n",
      "Condition 1 violated: 323 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165550 with loss: 0.0003806897148024291\n",
      "Epoch 165560, Loss: 0.0003\n",
      "Epoch 165570, Loss: 0.0003\n",
      "Epoch 165580, Loss: 0.0003\n",
      "Epoch 165590, Loss: 0.0003\n",
      "Epoch 165600, Loss: 0.0003\n",
      "Epoch 165610, Loss: 0.0003\n",
      "Epoch 165620, Loss: 0.0013\n",
      "Epoch 165630, Loss: 0.0008\n",
      "Epoch 165640, Loss: 0.0004\n",
      "Epoch 165650, Loss: 0.0005\n",
      "Condition 1 satisfied: 79 violations within the margin of 300\n",
      "Condition 1 violated: 453 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165650 with loss: 0.0005317466566339135\n",
      "Epoch 165660, Loss: 0.0005\n",
      "Epoch 165670, Loss: 0.0004\n",
      "Epoch 165680, Loss: 0.0003\n",
      "Epoch 165690, Loss: 0.0003\n",
      "Epoch 165700, Loss: 0.0002\n",
      "Epoch 165710, Loss: 0.0002\n",
      "Epoch 165720, Loss: 0.0002\n",
      "Epoch 165730, Loss: 0.0002\n",
      "Epoch 165740, Loss: 0.0002\n",
      "Epoch 165750, Loss: 0.0005\n",
      "Condition 1 satisfied: 68 violations within the margin of 300\n",
      "Condition 1 violated: 373 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165750 with loss: 0.0005148115451447666\n",
      "Epoch 165760, Loss: 0.0014\n",
      "Epoch 165770, Loss: 0.0005\n",
      "Epoch 165780, Loss: 0.0003\n",
      "Epoch 165790, Loss: 0.0003\n",
      "Epoch 165800, Loss: 0.0002\n",
      "Epoch 165810, Loss: 0.0002\n",
      "Epoch 165820, Loss: 0.0003\n",
      "Epoch 165830, Loss: 0.0004\n",
      "Epoch 165840, Loss: 0.0004\n",
      "Epoch 165850, Loss: 0.0005\n",
      "Condition 1 satisfied: 67 violations within the margin of 300\n",
      "Condition 1 violated: 463 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165850 with loss: 0.00048574869288131595\n",
      "Epoch 165860, Loss: 0.0004\n",
      "Epoch 165870, Loss: 0.0013\n",
      "Epoch 165880, Loss: 0.0041\n",
      "Epoch 165890, Loss: 0.0562\n",
      "Epoch 165900, Loss: 0.0054\n",
      "Epoch 165910, Loss: 0.0048\n",
      "Epoch 165920, Loss: 0.0023\n",
      "Epoch 165930, Loss: 0.0012\n",
      "Epoch 165940, Loss: 0.0009\n",
      "Epoch 165950, Loss: 0.0008\n",
      "Condition 1 satisfied: 167 violations within the margin of 300\n",
      "Condition 1 violated: 539 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 165950 with loss: 0.0007742704474367201\n",
      "Epoch 165960, Loss: 0.0006\n",
      "Epoch 165970, Loss: 0.0005\n",
      "Epoch 165980, Loss: 0.0006\n",
      "Epoch 165990, Loss: 0.0005\n",
      "Epoch 166000, Loss: 0.0005\n",
      "Epoch 166010, Loss: 0.0004\n",
      "Epoch 166020, Loss: 0.0004\n",
      "Epoch 166030, Loss: 0.0004\n",
      "Epoch 166040, Loss: 0.0005\n",
      "Epoch 166050, Loss: 0.0006\n",
      "Condition 1 satisfied: 35 violations within the margin of 300\n",
      "Condition 1 violated: 494 states have B(x) > 0, exceeding the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "Resuming training from epoch 166050 with loss: 0.0005799462669529021\n",
      "Epoch 166060, Loss: 0.0004\n",
      "Epoch 166070, Loss: 0.0006\n",
      "Epoch 166080, Loss: 0.0004\n",
      "Epoch 166090, Loss: 0.0004\n",
      "Epoch 166100, Loss: 0.0003\n",
      "Epoch 166110, Loss: 0.0005\n",
      "Epoch 166120, Loss: 0.0010\n",
      "Epoch 166130, Loss: 0.0004\n",
      "Epoch 166140, Loss: 0.0004\n",
      "Epoch 166150, Loss: 0.0004\n",
      "Condition 1 satisfied: 279 violations within the margin of 300\n",
      "Condition 1 satisfied: 264 violations within the margin of 300\n",
      "Condition 3 violated: 0 states found.\n",
      "no violating states\n",
      "goood enough\n"
     ]
    }
   ],
   "source": [
    "cond1 = verify_safe_states(barrier_net, states_safe)\n",
    "cond2 =  verify_unsafe_states(barrier_net, states_unsafe)\n",
    "safe_violating_states = []\n",
    "unsafe_violating_states = []\n",
    "checkpoint_path = 'checkpoint9.pth'\n",
    "checkpoint_path_save = 'checkpoint9.pth'\n",
    "\n",
    "for i in range(200):\n",
    "    count = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        barrier_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Continue from the next epoch\n",
    "        print(f\"Resuming training from epoch {start_epoch} with loss: {checkpoint['loss']}\")\n",
    "    for epoch in range(start_epoch, start_epoch+100):\n",
    "        # Forward pass: Compute B(x) for safe and unsafe states\n",
    "        B_s = barrier_net(states_safe)\n",
    "        B_u = barrier_net(states_unsafe)\n",
    "\n",
    "        # Compute Lf B(x) approximation for safe states\n",
    "        Lf_B_s = compute_Lf_B_approx(barrier_net, states_safe, next_states_safe, delta_t)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = barrier_loss(B_s, B_u, Lf_B_s, gamma, ws, wu, wl)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Save model and optimizer state every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,  # current epoch\n",
    "                'model_state_dict': barrier_net.state_dict(),  # model parameters\n",
    "                'optimizer_state_dict': optimizer.state_dict(),  # optimizer parameters\n",
    "                'loss': loss.item(),  # last loss value\n",
    "            }, checkpoint_path_save)\n",
    "        \n",
    "    cond1 = verify_safe_states(barrier_net, states_safe)\n",
    "\n",
    "    # Condition 2: Verify unsafe states\n",
    "    cond2 = verify_unsafe_states(barrier_net, states_unsafe)\n",
    "\n",
    "    # Condition 3: Verify Lie derivative for boundary states\n",
    "    grid_states = create_grid_around_boundary(states_safe, threshold=0.1, delta=0.05)\n",
    "\n",
    "    violating_states = verify_lie_derivative(barrier_net, grid_states, dynamics_model)\n",
    "    violating_states_np = np.array([state.detach().numpy() for state in violating_states])\n",
    "    if violating_states_np.size > 0:\n",
    "        violating_labels = knn_labeling(training_states, training_labels, violating_states_np)\n",
    "\n",
    "        # Split based on predicted labels\n",
    "        safe_violating_states = violating_states_np[violating_labels == 1]\n",
    "        unsafe_violating_states = violating_states_np[violating_labels == 0]\n",
    "\n",
    "\n",
    "    # Update training sets\n",
    "    if len(safe_violating_states) > 0:\n",
    "        safe_violating_states = torch.tensor(safe_violating_states, dtype=torch.float32)\n",
    "        states_safe = torch.cat([states_safe, safe_violating_states], dim=0)\n",
    "        states_safe = unique_tensor_rows(states_safe)\n",
    "        next_states_safe = states_safe[1:]  # All but the first state\n",
    "        states_safe = states_safe[:-1]  # All but the last state\n",
    "\n",
    "    if len(unsafe_violating_states) > 0:\n",
    "        unsafe_violating_states = torch.tensor(unsafe_violating_states, dtype=torch.float32)\n",
    "        states_unsafe = torch.cat([states_unsafe, unsafe_violating_states], dim=0)\n",
    "        states_unsafe = unique_tensor_rows(states_unsafe)\n",
    "    else:\n",
    "        print(\"no violating states\")\n",
    "    if cond1:\n",
    "        count+=1\n",
    "    if cond2:\n",
    "        count+=1\n",
    "    if violating_states_np.size < 60:\n",
    "        count+=1\n",
    "    if count > 2 :\n",
    "        print(\"goood enough\")\n",
    "        break\n",
    "        \n",
    "    # Print out the labels for violating states\n",
    "    # for i, state in enumerate(violating_states):\n",
    "    #     print(f\"State {state}: Label {violating_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse150b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
